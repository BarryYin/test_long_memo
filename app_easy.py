import streamlit as st
import json
import os
import yaml
from openai import OpenAI

# --- Configuration & Setup ---
st.set_page_config(page_title="Collection Agent (Easy Mode)", layout="wide")

# Configure OpenAI
api_key = os.getenv("OPENAI_API_KEY")
if not api_key and "OPENAI_API_KEY" in st.secrets:
    api_key = st.secrets["OPENAI_API_KEY"]

if not api_key:
    st.error("OpenAI API Key is missing. Please set it in environment variables or .streamlit/secrets.toml")
    st.stop()
base_url = os.getenv("OPENAI_BASE_URL")

client = OpenAI(api_key=api_key, base_url=base_url)
MODEL_NAME = "gpt-4o-mini"

# --- Helper Functions ---
def load_config(config_path):
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def call_llm(prompt, system_prompt="You are a helpful assistant.", json_mode=False):
    try:
        kwargs = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7
        }
        if json_mode:
            kwargs["response_format"] = {"type": "json_object"}
            
        response = client.chat.completions.create(**kwargs)
        return response.choices[0].message.content.strip()
    except Exception as e:
        st.error(f"LLM Error: {e}")
        return "Error"

# --- Agent Layers (Adapted for Streamlit) ---

class Layer1StrategyManager:
    def __init__(self, config, history_logs):
        self.config = config
        self.history_logs = history_logs

    def generate_initial_strategy(self, customer_profile):
        system_prompt = "ä½ æ˜¯å‚¬æ”¶ç­–ç•¥ç»ç†ã€‚æ ¹æ®å®¢æˆ·ä¿¡æ¯ã€å†å²è®°å½•ä»¥åŠå…¬å¸çš„åŸºç¡€é…ç½®è§„åˆ™ï¼Œåˆ¶å®šä»Šå¤©çš„ä¸´æ—¶å‚¬æ”¶ç­–ç•¥ã€‚"
        user_prompt = f"""
        å®¢æˆ·èµ„æ–™ï¼š{json.dumps(customer_profile, ensure_ascii=False)}
        
        è¯·åˆ¶å®šä¸€ä¸ªâ€œä»Šæ—¥ä¸´æ—¶å‚¬æ”¶ç­–ç•¥â€ï¼ŒæŒ‡å¯¼å‚¬æ”¶å‘˜å¦‚ä½•ä¸è¯¥å®¢æˆ·æ²Ÿé€šã€‚
        æ³¨æ„ï¼Œå†å²è®°å½•æ˜¯éå¸¸é‡è¦çš„èµ„æ–™æ¥æºï¼Œè¯´æ˜æˆ‘ä»¬å·²ç»ä¸å®¢æˆ·äº¤æµè¿‡äº†ï¼Œä½†å®¢æˆ·æ²¡æœ‰è¿˜é’±ï¼Œç°åœ¨å¸Œæœ›æ˜¯å»¶ç»­ä¹‹å‰çš„å¯¹è¯ï¼Œç»§ç»­ç»™å®¢æˆ·æ–½åŠ å‹åŠ›ã€‚

        *** å…³é”®çº¦æŸ ***
        ä½ åˆ¶å®šçš„ç­–ç•¥å¿…é¡»ä¸¥æ ¼éµå®ˆâ€œå…¬å¸åŸºç¡€é…ç½®/è§„åˆ™â€ä¸­çš„â€œHARD RULESâ€ã€‚
        ä¾‹å¦‚ï¼šå¦‚æœè§„åˆ™ç¦æ­¢éƒ¨åˆ†è¿˜æ¬¾ï¼Œä½ çš„ç­–ç•¥ä¸­ç»å¯¹ä¸èƒ½å»ºè®®æˆ–æš—ç¤ºå¯ä»¥æ¥å—éƒ¨åˆ†è¿˜æ¬¾ã€‚
        
        è¯·å…ˆåˆ†æå†å²è®°å½•ï¼š
        å†å²è®°å½•ï¼š{json.dumps(self.history_logs, ensure_ascii=False)}
        åŸºäºæ­¤è®°å¿†ä½“ï¼Œæˆ‘ä»¬èƒ½æ€»ç»“å‡ºå®¢æˆ·çš„è¿˜æ¬¾æ„æ„¿ï¼Œä¸è¿˜æ¬¾èƒ½åŠ›å—ï¼Ÿä»¥åŠæˆ‘ä»¬æ¥ä¸‹é‡Œå‚¬æ”¶ç­–ç•¥æ˜¯ä»€ä¹ˆã€‚ç›®æ ‡æ˜¯å°½å¿«æ‹¿å›é’±ã€‚
        
        è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
        
        ã€å†å²åˆ†æã€‘
        ï¼ˆåœ¨æ­¤å¤„ç®€è¦åˆ†æå®¢æˆ·æ˜¨å¤©çš„æ€åº¦ã€æ‰¿è¯ºã€è¿˜æ¬¾èƒ½åŠ›å’Œæ„æ„¿ï¼‰
        
        ã€ä»Šæ—¥ä¸´æ—¶å‚¬æ”¶ç­–ç•¥ã€‘
        1. æ²Ÿé€šåŸºè°ƒï¼š...
        2. é‡ç‚¹å¼ºè°ƒçš„å†…å®¹ï¼š...
        """
        return call_llm(user_prompt, system_prompt)

    def update_strategy(self, current_strategy, feedback, chat_history, customer_profile, layer3_advice):
        system_prompt = "ä½ æ˜¯å‚¬æ”¶ç­–ç•¥ç»ç†ã€‚Layer 3 è¯„ä¼°è®¤ä¸ºå½“å‰ç­–ç•¥å›æ¬¾å¯èƒ½æ€§è¾ƒä½ï¼Œè¯·æ ¹æ®å»ºè®®è°ƒæ•´ç­–ç•¥ã€‚"
        user_prompt = f"""
        Layer 3 è¯„ä¼°ä¸å»ºè®®ï¼š
        {layer3_advice}
        
        å®¢æˆ·èµ„æ–™ï¼š{json.dumps(customer_profile, ensure_ascii=False)}
        å†å²è®°å½•ï¼š{json.dumps(self.history_logs, ensure_ascii=False)}
        å½“å‰ä¼šè¯å†å²ï¼š{json.dumps(chat_history, ensure_ascii=False, indent=2)}
        å½“å‰ç­–ç•¥ï¼š{current_strategy}

        è¯·æ ¹æ® Layer 3 çš„å»ºè®®æ–¹å‘ï¼Œé‡æ–°åˆ¶å®šä»Šå¤©çš„å‚¬æ”¶ç­–ç•¥ã€‚
        ç­–ç•¥åº”å…·ä½“æŒ‡å¯¼å‚¬æ”¶å‘˜å¦‚ä½•ä½¿ç”¨å»ºè®®çš„æ¿€åŠ±æˆ–æ–½å‹æ‰‹æ®µï¼ˆå¦‚é»‘åå•ã€æé¢ã€æŠ˜æ‰£ç­‰ï¼‰æ¥çªç ´å®¢æˆ·é˜²çº¿ã€‚
        
        *** å…³é”®çº¦æŸ ***
        ä¿®æ”¹åçš„ç­–ç•¥ä¾ç„¶å¿…é¡»ä¸¥æ ¼éµå®ˆâ€œå…¬å¸åŸºç¡€é…ç½®/è§„åˆ™â€ä¸­çš„â€œHARD RULESâ€ã€‚
        ä¾‹å¦‚ï¼šå¦‚æœè§„åˆ™ç¦æ­¢éƒ¨åˆ†è¿˜æ¬¾ï¼Œå³ä½¿ä¸ºäº†æ¿€åŠ±å®¢æˆ·ï¼Œä¹Ÿä¸èƒ½è¿åæ­¤è§„åˆ™ï¼ˆé™¤éç”³è¯·æŠ˜æ‰£æ˜¯å…è®¸çš„ä¾‹å¤–ï¼‰ã€‚
        
        è¯·ç›´æ¥è¾“å‡ºä¿®æ”¹åçš„æ–°ç­–ç•¥ã€‚
        """
        return call_llm(user_prompt, system_prompt)

class Layer2Executor:
    def __init__(self, config):
        self.config = config
        self.base_system_prompt = self.config.get('system_prompt', '')

    def execute(self, strategy, chat_history, user_input):
        # Clean up the base prompt
        cleaned_base_prompt = self.base_system_prompt.replace("You must output a JSON object", "")
        cleaned_base_prompt = cleaned_base_prompt.replace("Output Format", "")
        
        combined_system_prompt = f"""
        {cleaned_base_prompt}
        
        ---
        TODAY'S TEMPORARY STRATEGY (FROM MANAGER):
        {strategy}
        ---

        å…¬å¸åŸºç¡€é…ç½®/è§„åˆ™ï¼š
        {json.dumps(self.config, ensure_ascii=False, indent=2)}

        è¯·å…ˆä¾æ®å†å²èŠå¤©æƒ…å†µï¼Œæ€»ç»“ä¸‹è¿‡å»å®¢æˆ·çš„è¿˜æ¬¾è¡¨ç°ï¼Œå»¶ç»­ä¸å®¢æˆ·çš„å¯¹è¯ã€‚æ‹‰è¿›ä¸å®¢æˆ·çš„å…³ç³»ï¼Œå¹¶æé†’å®¢æˆ·ä»Šå¤©çš„å€Ÿæ¬¾è¯¥è¿˜é’±äº†ï¼

        # INSTRUCTIONS
        1. You must follow the "Hard Rules" in the configuration AND the "Temporary Strategy" above.
        2. If there is a conflict, the "Hard Rules" (like NO Partial Payments) take precedence.
        3. **LANGUAGE**: You MUST reply in CHINESE (ä¸­æ–‡).
        4. **FORMAT**: You MUST output a JSON object with two fields:
           - "thought": Your internal reasoning (analyze customer intent, check rules, decide strategy).
           - "response": The final message to send to the customer (in Chinese).
        5. **CONTEXT**: You have a history with this customer. Do NOT re-introduce yourself (e.g., "Hello, I am Cindy") unless it is the very first message of a new conversation or the customer asks. Treat this as an ongoing professional relationship.
        6. **TONE**: Be CONCISE and PROFESSIONAL. Avoid excessive politeness or flowery language. You are a debt collector, not a customer service representative. Get straight to the point about payment.
        """
        
        messages = [{"role": "system", "content": combined_system_prompt}]
        for msg in chat_history:
            messages.append({"role": msg['role'], "content": msg['content']})
        messages.append({"role": "user", "content": user_input})
        
        content = call_llm(user_input, combined_system_prompt, json_mode=True) # Using call_llm wrapper but passing prompt as user_input is tricky because call_llm constructs messages. 
        # Let's use client directly here or adjust call_llm. 
        # Adjusting logic below to use client directly for full control over messages list.
        
        try:
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=messages,
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            content = response.choices[0].message.content.strip()
            data = json.loads(content)
            return data.get('response', ''), data.get('thought', '')
        except Exception as e:
            return f"System Error: {e}", ""

class Layer3Evaluator:
    def evaluate(self, last_user_input, last_agent_response):
        system_prompt = """
        ä½ æ˜¯å‚¬æ”¶ç­–ç•¥çš„è¯„ä¼°ä¸“å®¶ã€‚
        ä½ çš„ä»»åŠ¡æ˜¯è¯„ä¼°å½“å‰ç­–ç•¥åœ¨å®¢æˆ·èº«ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯â€œå®¢æˆ·å›æ¬¾çš„å¯èƒ½æ€§â€ã€‚
        
        è¯·åˆ†æå®¢æˆ·çš„æœ€æ–°å›å¤ï¼Œåˆ¤æ–­æŒ‰ç…§å½“å‰ç­–ç•¥ç»§ç»­ä¸‹å»ï¼Œå®¢æˆ·å›æ¬¾çš„å¯èƒ½æ€§æ˜¯ å¤§(HIGH)ã€ä¸­(MEDIUM) è¿˜æ˜¯ å°(LOW)ã€‚
        
        å¦‚æœå›æ¬¾å¯èƒ½æ€§ä¸º å°(LOW)ï¼Œä½ éœ€è¦ç»™å‡ºç­–ç•¥è°ƒæ•´å»ºè®®ã€‚ä½ å¯ä»¥ä»ä»¥ä¸‹6ä¸ªç»´åº¦ä¸­é€‰æ‹©æœ€é€‚åˆå½“å‰æƒ…å†µçš„åˆ‡å…¥ç‚¹ï¼ˆæŒ‡å¯¼ç¬¬ä¸€å±‚ç­–ç•¥ä¿®æ”¹ï¼‰ï¼š
        1. å®¢æˆ·è¿˜é’±ï¼Œä¼šè·å¾—å¥½çš„ä¿¡ç”¨ï¼Œæå‡ä¼šå‘˜ç­‰çº§ï¼›
        2. ä¼šæå‡è´·æ¬¾é¢åº¦å’Œç¬”æ•°ï¼›
        3. ä¼šè·å¾—è¿˜æ¬¾æŠ˜æ‰£ï¼›
        4. ä¼šå½±å“ä¿¡ç”¨åˆ†ï¼Œé™ä½è´·æ¬¾é¢åº¦ï¼›
        5. åç»­è´·æ¬¾ä¼šå¾ˆå›°éš¾ï¼Œæˆ‘ä»¬ä¼šåœæ‰ä¸å®¢æˆ·çš„åˆä½œï¼›
        6. æ‹‰å…¥é»‘åå•ï¼Œå®¢æˆ·çš„è´·æ¬¾è¡Œä¸ºå—é™ï¼Œä¸åªæ˜¯åœ¨æˆ‘ä»¬è¿™ä¸èƒ½å€Ÿæ¬¾ï¼Œåœ¨å“ªé‡Œéƒ½ä¸èƒ½å€Ÿæ¬¾ã€‚

        è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
        ã€åˆ†æã€‘ç®€è¦åˆ†æå®¢æˆ·çš„æŠ—æ‹’ç‚¹æˆ–å›°éš¾ï¼Œä»¥åŠå½“å‰ç­–ç•¥ä¸ºä½•æ— æ•ˆã€‚
        ã€å›æ¬¾å¯èƒ½æ€§ã€‘HIGH / MEDIUM / LOW
        ã€å»ºè®®æ–¹å‘ã€‘(å¦‚æœå¯èƒ½æ€§ä¸ºLOWï¼Œè¯·ä»ä¸Šè¿°6ç‚¹ä¸­é€‰æ‹©1-2ç‚¹å»ºè®®ï¼›å¦åˆ™ç•™ç©º)
        """
        user_prompt = f"Agent Response: {last_agent_response}\nCustomer Input: {last_user_input}"
        return call_llm(user_prompt, system_prompt)

# --- Main App Logic ---

def main():
    # Sidebar: Configuration
    st.sidebar.title("âš™ï¸ Configuration")
    
    # Config File Selection
    config_files = [f for f in os.listdir("configs") if f.endswith(".yaml")]
    selected_config = st.sidebar.selectbox("Select Config", config_files, index=config_files.index("T0.yaml") if "T0.yaml" in config_files else 0)
    
    if selected_config:
        config = load_config(os.path.join("configs", selected_config))
    else:
        st.error("No config file found!")
        return

    # Customer Profile
    st.sidebar.subheader("Customer Profile")
    default_profile = {
        "name": "LESTARI",
        "amount": "Rp 1.250.000",
        "due_date": "2025-12-17",
        "current_time": "2025-12-17",
        "gender": "Male",
        "age": "30",
        "frenquency_borrow":"often",
        "payment_timelyness":"sometimes late",
        "meaber_level": "high"
    }
    profile_str = st.sidebar.text_area("Edit Profile (JSON)", json.dumps(default_profile, indent=2, ensure_ascii=False), height=250)
    try:
        customer_profile = json.loads(profile_str)
    except:
        st.sidebar.error("Invalid JSON in Profile")
        customer_profile = default_profile

    # History Logs
    st.sidebar.subheader("History Logs")
    default_history = '''
   ã€è¿‡å»èŠå¤©æ€»ç»“ã€‘
åœ¨ä¸Šä¸€è½®å‚¬æ”¶ä¸­å·²åšå‡ºæ˜ç¡®ã€å…·ä½“çš„è¿˜æ¬¾æ‰¿è¯ºï¼ˆå¦‚â€œä»Šå¤©ä¸‹åˆ3ç‚¹å‡†æ—¶è¿˜â€ï¼‰ï¼Œä½†åˆ°è¾¾æ‰¿è¯ºæ—¶é—´ç‚¹åï¼Œä¸ä»…æœªè¿˜æ¬¾ï¼Œä¸”å¯¹åç»­æ‰€æœ‰æé†’æ¶ˆæ¯å·²è¯»ä¸å›ï¼Œé™·å…¥å®Œå…¨é™é»˜ã€‚â€
    '''
    history_logs = st.sidebar.text_area("Edit History Logs", default_history, height=200)
    
    # Initialize Session State
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "strategy" not in st.session_state:
        st.session_state.strategy = None
    if "layer1_analysis" not in st.session_state:
        st.session_state.layer1_analysis = None

    # Reset Button
    if st.sidebar.button("Reset Session"):
        st.session_state.messages = []
        st.session_state.strategy = None
        st.session_state.layer1_analysis = None
        st.rerun()

        # --- Main Area ---
    st.title("ğŸ¤– Collection Agent (Easy Mode)")

    # Layout: 2 Columns
    # Col 1: Chat (60%)
    # Col 2: Strategy & Analysis (40%)
    col_chat, col_info = st.columns([2, 1.2])

    # --- Right Column: Strategy & Analysis ---
    with col_info:
        st.subheader("ğŸ§  Agent Brain (Strategy & Analysis)")
        
        # 1. Global Strategy (Layer 1)
        with st.expander("ğŸ“‹ Daily Strategy (Layer 1)", expanded=True):
            if not st.session_state.strategy:
                # Auto-initialize: Generate strategy and opening message immediately
                with st.spinner("Layer 1 Manager is analyzing history and generating strategy..."):
                    layer1 = Layer1StrategyManager(config, [history_logs])
                    full_strategy_output = layer1.generate_initial_strategy(customer_profile)
                    st.session_state.strategy = full_strategy_output
                    
                    # Generate opening message
                    layer2 = Layer2Executor(config)
                    opening_instruction = "(System Instruction: Start the conversation now. If this is a new contact, introduce yourself. If there is history, continue naturally based on the strategy.)"
                    opening_response, thought = layer2.execute(full_strategy_output, [], opening_instruction)
                    
                    st.session_state.messages.append({"role": "assistant", "content": opening_response, "thought": thought})
                    st.rerun()
            else:
                st.info(st.session_state.strategy)
                
                # Add a button to force regenerate strategy if needed
                if st.button("Regenerate Strategy"):
                    st.session_state.strategy = None
                    st.session_state.messages = [] # Also clear messages to restart conversation
                    st.rerun()
        
        st.divider()
        st.markdown("**Thinking Process Log**")

    # --- Left Column: Chat Interface ---
    with col_chat:
        st.subheader("ğŸ’¬ Chat Interface")

    # --- Render History ---
    for i, msg in enumerate(st.session_state.messages):
        # Chat Content (Col 1)
        with col_chat:
            with st.chat_message(msg["role"]):
                st.write(msg["content"])
        
        # Analysis Content (Col 2)
        if msg["role"] == "assistant":
            with col_info:
                st.markdown(f"**Turn {i+1} Analysis**")
                
                # Layer 3
                if "layer3_evaluation" in msg and msg["layer3_evaluation"]:
                    with st.expander("ğŸ›¡ï¸ Layer 3 Evaluation", expanded=False):
                        st.caption(msg["layer3_evaluation"])
                
                # Layer 1 Update Event
                if "layer1_update" in msg and msg["layer1_update"]:
                    st.warning(f"ğŸ”„ Strategy Updated at Turn {i+1}")
                    with st.expander("View New Strategy"):
                        st.caption(msg["layer1_update"])

                # Layer 2 Thought
                if "thought" in msg and msg["thought"]:
                    with st.expander("ğŸ’­ Layer 2 Thought", expanded=False):
                        st.caption(msg["thought"])
                
                st.divider()

    # --- User Input Handling ---
    if prompt := st.chat_input("Type your reply here..."):
        # 1. Render User Message immediately
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with col_chat:
            with st.chat_message("user"):
                st.write(prompt)

        # 2. Process Assistant Response
        # Initialize Layers
        layer1 = Layer1StrategyManager(config, [history_logs])
        layer2 = Layer2Executor(config)
        layer3 = Layer3Evaluator()

        # --- Layer 3: Evaluation ---
        last_agent_response = st.session_state.messages[-2]['content'] if len(st.session_state.messages) > 1 else ""
        
        with col_info:
            st.markdown(f"**Current Turn Analysis**")
            with st.spinner("ğŸ›¡ï¸ Layer 3 Evaluating..."):
                evaluation_output = layer3.evaluate(prompt, last_agent_response)
            
            with st.expander("ğŸ›¡ï¸ Layer 3 Evaluation", expanded=True):
                st.caption(evaluation_output)

        # Check for LOW probability trigger
        is_low_prob = "LOW" in evaluation_output or "å¯èƒ½æ€§ã€‘LOW" in evaluation_output
        
        layer1_update_text = None
        if is_low_prob:
            with col_info:
                with st.spinner("âš ï¸ Low probability! Updating Strategy..."):
                    new_strategy = layer1.update_strategy(
                        st.session_state.strategy, 
                        prompt, 
                        st.session_state.messages,
                        customer_profile,
                        evaluation_output
                    )
                    st.session_state.strategy = new_strategy
                    layer1_update_text = new_strategy
                    st.warning("ğŸ”„ Strategy Updated!")
                    with st.expander("View New Strategy"):
                        st.caption(new_strategy)

        # --- Layer 2: Execution ---
        with col_info:
            with st.spinner("ğŸ’­ Layer 2 Thinking..."):
                response, thought = layer2.execute(
                    st.session_state.strategy, 
                    st.session_state.messages[:-1], 
                    prompt
                )
                with st.expander("ğŸ’­ Layer 2 Thought", expanded=True):
                    st.caption(thought)
                st.divider()

        # --- Output Response ---
        with col_chat:
            with st.chat_message("assistant"):
                st.write(response)
        
        # Save to History
        st.session_state.messages.append({
            "role": "assistant", 
            "content": response, 
            "thought": thought,
            "layer3_evaluation": evaluation_output,
            "layer1_update": layer1_update_text
        })

    



if __name__ == "__main__":
    main()
